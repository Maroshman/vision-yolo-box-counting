{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d325e6",
   "metadata": {},
   "source": [
    "# YOLO Box Detection Training Notebook\n",
    "\n",
    "This notebook provides a comprehensive guide for training a YOLO model for box detection and counting using Roboflow datasets.\n",
    "\n",
    "## Overview\n",
    "- **Objective**: Train a YOLO model to detect and count boxes in images\n",
    "- **Framework**: Ultralytics YOLO v8\n",
    "- **Data Source**: Roboflow datasets\n",
    "- **Output**: Trained model for box detection and counting\n",
    "\n",
    "## Workflow\n",
    "1. Install and import required libraries\n",
    "2. Set up Roboflow API connection\n",
    "3. Download and prepare dataset\n",
    "4. Configure YOLO model\n",
    "5. Train the model\n",
    "6. Test on sample images\n",
    "7. Implement box counting logic\n",
    "8. Evaluate performance\n",
    "9. Deploy for real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d9e191",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "First, let's install and import all the necessary libraries for YOLO model training and box detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b5b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install ultralytics roboflow opencv-python matplotlib seaborn pandas numpy pillow pyyaml\n",
    "\n",
    "# Verify PyTorch installation\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f5443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import json\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import YOLO and Roboflow\n",
    "from ultralytics import YOLO\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c782cfe",
   "metadata": {},
   "source": [
    "## 2. Set Up Roboflow API Connection\n",
    "\n",
    "Configure your Roboflow API credentials to access datasets and model management features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4847c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roboflow Configuration\n",
    "# Get your API key from: https://roboflow.com/\n",
    "\n",
    "# Option 1: Set your credentials here (not recommended for production)\n",
    "ROBOFLOW_API_KEY = \"your_api_key_here\"  # Replace with your actual API key\n",
    "WORKSPACE_NAME = \"your_workspace\"       # Replace with your workspace name\n",
    "PROJECT_NAME = \"box-detection\"          # Replace with your project name\n",
    "VERSION = 1                             # Dataset version\n",
    "\n",
    "# Option 2: Load from environment variables (recommended)\n",
    "# import os\n",
    "# ROBOFLOW_API_KEY = os.getenv(\"ROBOFLOW_API_KEY\")\n",
    "# WORKSPACE_NAME = os.getenv(\"ROBOFLOW_WORKSPACE\")\n",
    "# PROJECT_NAME = os.getenv(\"ROBOFLOW_PROJECT\", \"box-detection\")\n",
    "\n",
    "# Initialize Roboflow client\n",
    "try:\n",
    "    rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
    "    workspace = rf.workspace(WORKSPACE_NAME)\n",
    "    project = workspace.project(PROJECT_NAME)\n",
    "    print(f\"‚úÖ Connected to Roboflow project: {WORKSPACE_NAME}/{PROJECT_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error connecting to Roboflow: {e}\")\n",
    "    print(\"Please check your API key and project details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacded8d",
   "metadata": {},
   "source": [
    "## 3. Download and Prepare Dataset\n",
    "\n",
    "Download the box detection dataset from Roboflow and prepare it for YOLO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417278a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from Roboflow\n",
    "print(\"üì• Downloading dataset...\")\n",
    "\n",
    "try:\n",
    "    # Get dataset version\n",
    "    version = project.version(VERSION)\n",
    "    \n",
    "    # Download in YOLO format\n",
    "    dataset = version.download(\"yolov8\", location=\"./datasets\")\n",
    "    \n",
    "    print(f\"‚úÖ Dataset downloaded successfully!\")\n",
    "    print(f\"üìÅ Dataset location: {dataset.location}\")\n",
    "    \n",
    "    # Store dataset path for later use\n",
    "    DATASET_PATH = dataset.location\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error downloading dataset: {e}\")\n",
    "    # Fallback to manual path if download fails\n",
    "    DATASET_PATH = \"./datasets/box-detection-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b90de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset structure\n",
    "print(\"üîç Exploring dataset structure...\")\n",
    "\n",
    "# List dataset contents\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    for item in os.listdir(DATASET_PATH):\n",
    "        item_path = os.path.join(DATASET_PATH, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            count = len([f for f in os.listdir(item_path) if f.endswith(('.jpg', '.png', '.txt'))])\n",
    "            print(f\"üìÇ {item}: {count} files\")\n",
    "        else:\n",
    "            print(f\"üìÑ {item}\")\n",
    "    \n",
    "    # Load data.yaml to check configuration\n",
    "    yaml_path = os.path.join(DATASET_PATH, \"data.yaml\")\n",
    "    if os.path.exists(yaml_path):\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            data_config = yaml.safe_load(f)\n",
    "        \n",
    "        print(\"\\nüìã Dataset Configuration:\")\n",
    "        print(f\"  Classes: {data_config.get('nc', 'Unknown')}\")\n",
    "        print(f\"  Class names: {data_config.get('names', 'Unknown')}\")\n",
    "        print(f\"  Train path: {data_config.get('train', 'Unknown')}\")\n",
    "        print(f\"  Validation path: {data_config.get('val', 'Unknown')}\")\n",
    "        \n",
    "        # Store for later use\n",
    "        NUM_CLASSES = data_config.get('nc', 1)\n",
    "        CLASS_NAMES = data_config.get('names', ['box'])\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è data.yaml not found\")\n",
    "        NUM_CLASSES = 1\n",
    "        CLASS_NAMES = ['box']\n",
    "else:\n",
    "    print(\"‚ùå Dataset path not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d89f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from the dataset\n",
    "def visualize_sample_images(dataset_path, num_samples=6):\n",
    "    \"\"\"Visualize sample images with their annotations\"\"\"\n",
    "    \n",
    "    train_images_path = os.path.join(dataset_path, \"train\", \"images\")\n",
    "    train_labels_path = os.path.join(dataset_path, \"train\", \"labels\")\n",
    "    \n",
    "    if not os.path.exists(train_images_path):\n",
    "        print(\"‚ùå Training images not found\")\n",
    "        return\n",
    "    \n",
    "    # Get sample images\n",
    "    image_files = [f for f in os.listdir(train_images_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "    sample_files = image_files[:num_samples]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, img_file in enumerate(sample_files):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "            \n",
    "        # Load image\n",
    "        img_path = os.path.join(train_images_path, img_file)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load corresponding label\n",
    "        label_file = img_file.replace('.jpg', '.txt').replace('.png', '.txt').replace('.jpeg', '.txt')\n",
    "        label_path = os.path.join(train_labels_path, label_file)\n",
    "        \n",
    "        # Draw bounding boxes if label exists\n",
    "        if os.path.exists(label_path):\n",
    "            h, w = image.shape[:2]\n",
    "            with open(label_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            box_count = 0\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    _, x_center, y_center, width, height = map(float, parts[:5])\n",
    "                    \n",
    "                    # Convert YOLO format to pixel coordinates\n",
    "                    x1 = int((x_center - width/2) * w)\n",
    "                    y1 = int((y_center - height/2) * h)\n",
    "                    x2 = int((x_center + width/2) * w)\n",
    "                    y2 = int((y_center + height/2) * h)\n",
    "                    \n",
    "                    # Draw bounding box\n",
    "                    cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                    box_count += 1\n",
    "            \n",
    "            title = f\"{img_file}\\n{box_count} boxes\"\n",
    "        else:\n",
    "            title = f\"{img_file}\\nNo labels\"\n",
    "        \n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(title, fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(sample_files), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"Sample Training Images with Box Annotations\", fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples\n",
    "visualize_sample_images(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab5850",
   "metadata": {},
   "source": [
    "## 4. Configure YOLO Model\n",
    "\n",
    "Set up the YOLO model configuration including architecture selection and training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4389ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO Model Configuration\n",
    "MODEL_SIZE = \"yolov8n\"  # Options: yolov8n, yolov8s, yolov8m, yolov8l, yolov8x\n",
    "                        # n=nano (fastest), s=small, m=medium, l=large, x=extra large (most accurate)\n",
    "\n",
    "# Training parameters\n",
    "TRAINING_CONFIG = {\n",
    "    'epochs': 100,           # Number of training epochs\n",
    "    'batch': 16,            # Batch size (adjust based on GPU memory)\n",
    "    'imgsz': 640,           # Image size for training\n",
    "    'patience': 50,         # Early stopping patience\n",
    "    'save_period': 10,      # Save model every N epochs\n",
    "    'workers': 8,           # Number of dataloader workers\n",
    "    'optimizer': 'auto',    # Optimizer (auto, SGD, Adam, AdamW, etc.)\n",
    "    'lr0': 0.01,           # Initial learning rate\n",
    "    'lrf': 0.01,           # Final learning rate (lr0 * lrf)\n",
    "    'momentum': 0.937,      # SGD momentum/Adam beta1\n",
    "    'weight_decay': 0.0005, # Optimizer weight decay\n",
    "    'warmup_epochs': 3,     # Warmup epochs\n",
    "    'warmup_momentum': 0.8, # Warmup initial momentum\n",
    "    'box': 7.5,            # Box loss gain\n",
    "    'cls': 0.5,            # Class loss gain\n",
    "    'dfl': 1.5,            # DFL loss gain\n",
    "    'pose': 12.0,          # Pose loss gain (pose models only)\n",
    "    'kobj': 2.0,           # Keypoint obj loss gain (pose models only)\n",
    "    'label_smoothing': 0.0, # Label smoothing (fraction)\n",
    "    'nbs': 64,             # Nominal batch size\n",
    "    'hsv_h': 0.015,        # Image HSV-Hue augmentation (fraction)\n",
    "    'hsv_s': 0.7,          # Image HSV-Saturation augmentation (fraction)\n",
    "    'hsv_v': 0.4,          # Image HSV-Value augmentation (fraction)\n",
    "    'degrees': 0.0,        # Image rotation (+/- deg)\n",
    "    'translate': 0.1,      # Image translation (+/- fraction)\n",
    "    'scale': 0.5,          # Image scale (+/- gain)\n",
    "    'shear': 0.0,          # Image shear (+/- deg)\n",
    "    'perspective': 0.0,    # Image perspective (+/- fraction), range 0-0.001\n",
    "    'flipud': 0.0,         # Image flip up-down (probability)\n",
    "    'fliplr': 0.5,         # Image flip left-right (probability)\n",
    "    'mosaic': 1.0,         # Image mosaic (probability)\n",
    "    'mixup': 0.0,          # Image mixup (probability)\n",
    "    'copy_paste': 0.0,     # Segment copy-paste (probability)\n",
    "}\n",
    "\n",
    "print(\"üîß Model Configuration:\")\n",
    "print(f\"  Model: {MODEL_SIZE}\")\n",
    "print(f\"  Epochs: {TRAINING_CONFIG['epochs']}\")\n",
    "print(f\"  Batch size: {TRAINING_CONFIG['batch']}\")\n",
    "print(f\"  Image size: {TRAINING_CONFIG['imgsz']}\")\n",
    "print(f\"  Learning rate: {TRAINING_CONFIG['lr0']}\")\n",
    "print(f\"  Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Initialize model\n",
    "model = YOLO(f'{MODEL_SIZE}.pt')  # Load pretrained model\n",
    "print(f\"‚úÖ Loaded {MODEL_SIZE} model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d9c4cd",
   "metadata": {},
   "source": [
    "## 5. Train the YOLO Model\n",
    "\n",
    "Start training the YOLO model on the box detection dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e2841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting model training...\")\n",
    "print(\"This may take several hours depending on your dataset size and hardware.\")\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    results = model.train(\n",
    "        data=os.path.join(DATASET_PATH, \"data.yaml\"),\n",
    "        **TRAINING_CONFIG\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "    print(f\"üìÅ Results saved to: {results.save_dir}\")\n",
    "    \n",
    "    # Store the best model path\n",
    "    BEST_MODEL_PATH = os.path.join(results.save_dir, \"weights\", \"best.pt\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    # Set a fallback path\n",
    "    BEST_MODEL_PATH = f\"runs/detect/train/weights/best.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ec464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training results\n",
    "def plot_training_results(results_dir):\n",
    "    \"\"\"Plot training metrics from results\"\"\"\n",
    "    \n",
    "    results_csv = os.path.join(results_dir, \"results.csv\")\n",
    "    \n",
    "    if os.path.exists(results_csv):\n",
    "        # Load training results\n",
    "        df = pd.read_csv(results_csv)\n",
    "        df.columns = df.columns.str.strip()  # Remove any whitespace\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Plot loss curves\n",
    "        if 'train/box_loss' in df.columns:\n",
    "            axes[0, 0].plot(df['epoch'], df['train/box_loss'], label='Train Box Loss', color='blue')\n",
    "            if 'val/box_loss' in df.columns:\n",
    "                axes[0, 0].plot(df['epoch'], df['val/box_loss'], label='Val Box Loss', color='red')\n",
    "            axes[0, 0].set_title('Box Loss')\n",
    "            axes[0, 0].set_xlabel('Epoch')\n",
    "            axes[0, 0].set_ylabel('Loss')\n",
    "            axes[0, 0].legend()\n",
    "            axes[0, 0].grid(True)\n",
    "        \n",
    "        # Plot class loss\n",
    "        if 'train/cls_loss' in df.columns:\n",
    "            axes[0, 1].plot(df['epoch'], df['train/cls_loss'], label='Train Class Loss', color='blue')\n",
    "            if 'val/cls_loss' in df.columns:\n",
    "                axes[0, 1].plot(df['epoch'], df['val/cls_loss'], label='Val Class Loss', color='red')\n",
    "            axes[0, 1].set_title('Class Loss')\n",
    "            axes[0, 1].set_xlabel('Epoch')\n",
    "            axes[0, 1].set_ylabel('Loss')\n",
    "            axes[0, 1].legend()\n",
    "            axes[0, 1].grid(True)\n",
    "        \n",
    "        # Plot mAP\n",
    "        if 'metrics/mAP50(B)' in df.columns:\n",
    "            axes[1, 0].plot(df['epoch'], df['metrics/mAP50(B)'], label='mAP@0.5', color='green')\n",
    "            if 'metrics/mAP50-95(B)' in df.columns:\n",
    "                axes[1, 0].plot(df['epoch'], df['metrics/mAP50-95(B)'], label='mAP@0.5:0.95', color='orange')\n",
    "            axes[1, 0].set_title('Mean Average Precision')\n",
    "            axes[1, 0].set_xlabel('Epoch')\n",
    "            axes[1, 0].set_ylabel('mAP')\n",
    "            axes[1, 0].legend()\n",
    "            axes[1, 0].grid(True)\n",
    "        \n",
    "        # Plot precision and recall\n",
    "        if 'metrics/precision(B)' in df.columns:\n",
    "            axes[1, 1].plot(df['epoch'], df['metrics/precision(B)'], label='Precision', color='purple')\n",
    "            if 'metrics/recall(B)' in df.columns:\n",
    "                axes[1, 1].plot(df['epoch'], df['metrics/recall(B)'], label='Recall', color='brown')\n",
    "            axes[1, 1].set_title('Precision & Recall')\n",
    "            axes[1, 1].set_xlabel('Epoch')\n",
    "            axes[1, 1].set_ylabel('Score')\n",
    "            axes[1, 1].legend()\n",
    "            axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('Training Results', fontsize=16, y=1.02)\n",
    "        plt.show()\n",
    "        \n",
    "        # Print final metrics\n",
    "        if len(df) > 0:\n",
    "            final_row = df.iloc[-1]\n",
    "            print(\"üìä Final Training Metrics:\")\n",
    "            if 'metrics/mAP50(B)' in df.columns:\n",
    "                print(f\"  mAP@0.5: {final_row['metrics/mAP50(B)']:.3f}\")\n",
    "            if 'metrics/mAP50-95(B)' in df.columns:\n",
    "                print(f\"  mAP@0.5:0.95: {final_row['metrics/mAP50-95(B)']:.3f}\")\n",
    "            if 'metrics/precision(B)' in df.columns:\n",
    "                print(f\"  Precision: {final_row['metrics/precision(B)']:.3f}\")\n",
    "            if 'metrics/recall(B)' in df.columns:\n",
    "                print(f\"  Recall: {final_row['metrics/recall(B)']:.3f}\")\n",
    "    else:\n",
    "        print(\"‚ùå Training results not found\")\n",
    "\n",
    "# Plot results if training completed\n",
    "try:\n",
    "    if 'results' in locals() and hasattr(results, 'save_dir'):\n",
    "        plot_training_results(results.save_dir)\n",
    "    else:\n",
    "        # Try to find the latest training run\n",
    "        runs_dir = \"runs/detect\"\n",
    "        if os.path.exists(runs_dir):\n",
    "            train_dirs = [d for d in os.listdir(runs_dir) if d.startswith('train')]\n",
    "            if train_dirs:\n",
    "                latest_dir = os.path.join(runs_dir, sorted(train_dirs)[-1])\n",
    "                plot_training_results(latest_dir)\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot training results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b531cd9",
   "metadata": {},
   "source": [
    "## 6. Test Model on Sample Images\n",
    "\n",
    "Load the trained model and test its detection capabilities on sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed316115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "try:\n",
    "    if os.path.exists(BEST_MODEL_PATH):\n",
    "        trained_model = YOLO(BEST_MODEL_PATH)\n",
    "        print(f\"‚úÖ Loaded trained model from: {BEST_MODEL_PATH}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Trained model not found, using pretrained model\")\n",
    "        trained_model = YOLO(f'{MODEL_SIZE}.pt')\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    trained_model = YOLO(f'{MODEL_SIZE}.pt')\n",
    "\n",
    "# Test on sample images\n",
    "def test_model_on_samples(model, dataset_path, num_samples=6, confidence=0.5):\n",
    "    \"\"\"Test the model on sample images\"\"\"\n",
    "    \n",
    "    # Try validation set first, then test set\n",
    "    for split in ['valid', 'test']:\n",
    "        test_images_path = os.path.join(dataset_path, split, \"images\")\n",
    "        if os.path.exists(test_images_path):\n",
    "            break\n",
    "    else:\n",
    "        # Fallback to training images\n",
    "        test_images_path = os.path.join(dataset_path, \"train\", \"images\")\n",
    "    \n",
    "    if not os.path.exists(test_images_path):\n",
    "        print(\"‚ùå No test images found\")\n",
    "        return\n",
    "    \n",
    "    # Get sample images\n",
    "    image_files = [f for f in os.listdir(test_images_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "    sample_files = image_files[:num_samples]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, img_file in enumerate(sample_files):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "            \n",
    "        # Load and process image\n",
    "        img_path = os.path.join(test_images_path, img_file)\n",
    "        image = cv2.imread(img_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Run inference\n",
    "        results = model(image_rgb, conf=confidence)\n",
    "        \n",
    "        # Count detections\n",
    "        box_count = 0\n",
    "        if results[0].boxes is not None:\n",
    "            box_count = len(results[0].boxes)\n",
    "            \n",
    "            # Draw bounding boxes\n",
    "            for box in results[0].boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "                conf = box.conf[0].cpu().numpy()\n",
    "                \n",
    "                # Draw rectangle\n",
    "                cv2.rectangle(image_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                \n",
    "                # Add confidence label\n",
    "                label = f\"Box: {conf:.2f}\"\n",
    "                cv2.putText(image_rgb, label, (x1, y1-10), \n",
    "                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display image\n",
    "        axes[i].imshow(image_rgb)\n",
    "        axes[i].set_title(f\"{img_file}\\\\nDetected: {box_count} boxes\", fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(sample_files), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"Model Predictions on Test Images\", fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    return len(sample_files)\n",
    "\n",
    "# Test the model\n",
    "num_tested = test_model_on_samples(trained_model, DATASET_PATH, confidence=0.5)\n",
    "print(f\"üîç Tested model on {num_tested} sample images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e5ef1d",
   "metadata": {},
   "source": [
    "## 7. Implement Box Detection and Counting\n",
    "\n",
    "Create comprehensive functions to detect boxes and implement accurate counting logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e65706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxCounter:\n",
    "    \"\"\"Advanced box detection and counting system\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, confidence=0.5, iou_threshold=0.45):\n",
    "        \"\"\"Initialize the box counter\"\"\"\n",
    "        self.model = YOLO(model_path)\n",
    "        self.confidence = confidence\n",
    "        self.iou_threshold = iou_threshold\n",
    "        \n",
    "    def detect_boxes(self, image, return_details=False):\n",
    "        \"\"\"\n",
    "        Detect boxes in an image\n",
    "        \n",
    "        Args:\n",
    "            image: Input image (numpy array or path)\n",
    "            return_details: Whether to return detailed detection info\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with detection results\n",
    "        \"\"\"\n",
    "        # Load image if path is provided\n",
    "        if isinstance(image, str):\n",
    "            image = cv2.imread(image)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Run inference\n",
    "        results = self.model(image, conf=self.confidence, iou=self.iou_threshold)\n",
    "        \n",
    "        # Extract detection information\n",
    "        detections = {\n",
    "            'count': 0,\n",
    "            'boxes': [],\n",
    "            'confidences': [],\n",
    "            'areas': [],\n",
    "            'centers': []\n",
    "        }\n",
    "        \n",
    "        if results[0].boxes is not None:\n",
    "            boxes = results[0].boxes\n",
    "            detections['count'] = len(boxes)\n",
    "            \n",
    "            for box in boxes:\n",
    "                # Get box coordinates\n",
    "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "                conf = float(box.conf[0].cpu().numpy())\n",
    "                \n",
    "                # Calculate additional metrics\n",
    "                area = (x2 - x1) * (y2 - y1)\n",
    "                center_x = (x1 + x2) // 2\n",
    "                center_y = (y1 + y2) // 2\n",
    "                \n",
    "                detections['boxes'].append([x1, y1, x2, y2])\n",
    "                detections['confidences'].append(conf)\n",
    "                detections['areas'].append(area)\n",
    "                detections['centers'].append([center_x, center_y])\n",
    "        \n",
    "        if return_details:\n",
    "            return detections\n",
    "        else:\n",
    "            return detections['count']\n",
    "    \n",
    "    def visualize_detections(self, image, detections=None, save_path=None):\n",
    "        \"\"\"\n",
    "        Visualize box detections on image\n",
    "        \n",
    "        Args:\n",
    "            image: Input image\n",
    "            detections: Detection results (optional, will compute if not provided)\n",
    "            save_path: Path to save visualization\n",
    "            \n",
    "        Returns:\n",
    "            Annotated image\n",
    "        \"\"\"\n",
    "        # Load image if path is provided\n",
    "        if isinstance(image, str):\n",
    "            image_path = image\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get detections if not provided\n",
    "        if detections is None:\n",
    "            detections = self.detect_boxes(image, return_details=True)\n",
    "        \n",
    "        # Create a copy for annotation\n",
    "        annotated_image = image.copy()\n",
    "        \n",
    "        # Draw bounding boxes\n",
    "        for i, (box, conf, area, center) in enumerate(zip(\n",
    "            detections['boxes'], \n",
    "            detections['confidences'], \n",
    "            detections['areas'],\n",
    "            detections['centers']\n",
    "        )):\n",
    "            x1, y1, x2, y2 = box\n",
    "            \n",
    "            # Color based on confidence (green for high, yellow for medium, red for low)\n",
    "            if conf > 0.7:\n",
    "                color = (0, 255, 0)  # Green\n",
    "            elif conf > 0.5:\n",
    "                color = (255, 255, 0)  # Yellow\n",
    "            else:\n",
    "                color = (255, 0, 0)  # Red\n",
    "            \n",
    "            # Draw rectangle\n",
    "            cv2.rectangle(annotated_image, (x1, y1), (x2, y2), color, 2)\n",
    "            \n",
    "            # Draw center point\n",
    "            cv2.circle(annotated_image, tuple(center), 3, color, -1)\n",
    "            \n",
    "            # Add label with confidence and box number\n",
    "            label = f\"Box {i+1}: {conf:.2f}\"\n",
    "            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "            cv2.rectangle(annotated_image, (x1, y1-label_size[1]-10), \n",
    "                         (x1+label_size[0], y1), color, -1)\n",
    "            cv2.putText(annotated_image, label, (x1, y1-5), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n",
    "        \n",
    "        # Add summary text\n",
    "        summary_text = f\"Total Boxes: {detections['count']}\"\n",
    "        if detections['confidences']:\n",
    "            avg_conf = np.mean(detections['confidences'])\n",
    "            summary_text += f\" | Avg Confidence: {avg_conf:.3f}\"\n",
    "        \n",
    "        cv2.putText(annotated_image, summary_text, (10, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 3)\n",
    "        cv2.putText(annotated_image, summary_text, (10, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "        \n",
    "        # Save if path provided\n",
    "        if save_path:\n",
    "            cv2.imwrite(save_path, cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        return annotated_image\n",
    "    \n",
    "    def batch_count(self, image_paths, save_results=True, output_dir=\"results\"):\n",
    "        \"\"\"\n",
    "        Count boxes in multiple images\n",
    "        \n",
    "        Args:\n",
    "            image_paths: List of image paths or directory path\n",
    "            save_results: Whether to save results\n",
    "            output_dir: Directory to save results\n",
    "            \n",
    "        Returns:\n",
    "            List of results for each image\n",
    "        \"\"\"\n",
    "        # Handle directory input\n",
    "        if isinstance(image_paths, str) and os.path.isdir(image_paths):\n",
    "            image_dir = image_paths\n",
    "            image_paths = []\n",
    "            for ext in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "                image_paths.extend(Path(image_dir).glob(f\"*{ext}\"))\n",
    "                image_paths.extend(Path(image_dir).glob(f\"*{ext.upper()}\"))\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Create output directory\n",
    "        if save_results:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        for i, img_path in enumerate(image_paths):\n",
    "            try:\n",
    "                print(f\"Processing {i+1}/{len(image_paths)}: {Path(img_path).name}\")\n",
    "                \n",
    "                # Detect boxes\n",
    "                detections = self.detect_boxes(str(img_path), return_details=True)\n",
    "                \n",
    "                # Create result record\n",
    "                result = {\n",
    "                    'image_path': str(img_path),\n",
    "                    'image_name': Path(img_path).name,\n",
    "                    'box_count': detections['count'],\n",
    "                    'avg_confidence': np.mean(detections['confidences']) if detections['confidences'] else 0,\n",
    "                    'max_confidence': max(detections['confidences']) if detections['confidences'] else 0,\n",
    "                    'min_confidence': min(detections['confidences']) if detections['confidences'] else 0,\n",
    "                    'total_area': sum(detections['areas']),\n",
    "                    'avg_box_area': np.mean(detections['areas']) if detections['areas'] else 0\n",
    "                }\\n                results.append(result)\\n                \\n                # Save visualization if requested\\n                if save_results:\\n                    img = cv2.imread(str(img_path))\\n                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\\n                    annotated = self.visualize_detections(img_rgb, detections)\\n                    \\n                    output_name = f\"{Path(img_path).stem}_detected.jpg\"\\n                    output_path = os.path.join(output_dir, output_name)\\n                    cv2.imwrite(output_path, cv2.cvtColor(annotated, cv2.COLOR_RGB2BGR))\\n                \\n            except Exception as e:\\n                print(f\"Error processing {img_path}: {e}\")\\n                result = {\\n                    'image_path': str(img_path),\\n                    'image_name': Path(img_path).name,\\n                    'box_count': 0,\\n                    'error': str(e)\\n                }\\n                results.append(result)\\n        \\n        # Save summary results\\n        if save_results and results:\\n            summary_path = os.path.join(output_dir, \"counting_results.json\")\\n            with open(summary_path, 'w') as f:\\n                json.dump(results, f, indent=2)\\n            \\n            # Create CSV summary\\n            df = pd.DataFrame(results)\\n            csv_path = os.path.join(output_dir, \"counting_summary.csv\")\\n            df.to_csv(csv_path, index=False)\\n            \\n            print(f\"‚úÖ Results saved to {output_dir}\")\\n        \\n        return results\\n\\n# Initialize the box counter with the trained model\\ntry:\\n    box_counter = BoxCounter(BEST_MODEL_PATH, confidence=0.5, iou_threshold=0.45)\\n    print(\"‚úÖ Box counter initialized with trained model\")\\nexcept:\\n    box_counter = BoxCounter(f'{MODEL_SIZE}.pt', confidence=0.5, iou_threshold=0.45)\\n    print(\"‚ö†Ô∏è Using pretrained model for box counter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaef10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the box counter on sample images\n",
    "def demo_box_counting(box_counter, dataset_path, num_samples=4):\n",
    "    \"\"\"Demonstrate box counting functionality\"\"\"\n",
    "    \n",
    "    # Get test images\n",
    "    test_images_path = os.path.join(dataset_path, \"valid\", \"images\")\n",
    "    if not os.path.exists(test_images_path):\n",
    "        test_images_path = os.path.join(dataset_path, \"train\", \"images\")\n",
    "    \n",
    "    if not os.path.exists(test_images_path):\n",
    "        print(\"‚ùå No test images found\")\n",
    "        return\n",
    "    \n",
    "    image_files = [f for f in os.listdir(test_images_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "    sample_files = image_files[:num_samples]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    total_boxes = 0\n",
    "    \n",
    "    for i, img_file in enumerate(sample_files):\n",
    "        img_path = os.path.join(test_images_path, img_file)\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(img_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Count boxes\n",
    "        detections = box_counter.detect_boxes(image_rgb, return_details=True)\n",
    "        count = detections['count']\n",
    "        total_boxes += count\n",
    "        \n",
    "        # Visualize\n",
    "        annotated = box_counter.visualize_detections(image_rgb, detections)\n",
    "        \n",
    "        # Display\n",
    "        axes[i].imshow(annotated)\n",
    "        axes[i].set_title(f\"{img_file}\\\\nBoxes: {count}\", fontsize=12)\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Print details\n",
    "        print(f\"üì¶ {img_file}: {count} boxes detected\")\n",
    "        if detections['confidences']:\n",
    "            print(f\"   Confidence range: {min(detections['confidences']):.3f} - {max(detections['confidences']):.3f}\")\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(sample_files), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Box Counting Demo - Total: {total_boxes} boxes\", fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    return total_boxes\n",
    "\n",
    "# Run the demo\n",
    "total_detected = demo_box_counting(box_counter, DATASET_PATH)\n",
    "print(f\"\\\\nüéØ Demo completed! Total boxes detected: {total_detected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef8431",
   "metadata": {},
   "source": [
    "## 8. Evaluate Model Performance\n",
    "\n",
    "Calculate comprehensive performance metrics including precision, recall, mAP, and counting accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af3525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance on validation set\n",
    "def evaluate_model_performance(model, dataset_path):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    \n",
    "    # Run validation using YOLO's built-in validation\n",
    "    print(\"üîç Running model validation...\")\n",
    "    \n",
    "    try:\\n        # Validate on the dataset\\n        val_results = model.val(data=os.path.join(dataset_path, \"data.yaml\"))\\n        \\n        print(\"üìä Validation Metrics:\")\\n        metrics = val_results.results_dict\\n        \\n        # Print key metrics\\n        if 'metrics/mAP50(B)' in metrics:\\n            print(f\"  mAP@0.5: {metrics['metrics/mAP50(B)']:.4f}\")\\n        if 'metrics/mAP50-95(B)' in metrics:\\n            print(f\"  mAP@0.5:0.95: {metrics['metrics/mAP50-95(B)']:.4f}\")\\n        if 'metrics/precision(B)' in metrics:\\n            print(f\"  Precision: {metrics['metrics/precision(B)']:.4f}\")\\n        if 'metrics/recall(B)' in metrics:\\n            print(f\"  Recall: {metrics['metrics/recall(B)']:.4f}\")\\n        \\n        return val_results\\n        \\n    except Exception as e:\\n        print(f\"‚ùå Validation failed: {e}\")\\n        return None\\n\\n# Custom counting accuracy evaluation\\ndef evaluate_counting_accuracy(box_counter, dataset_path, split='valid'):\\n    \"\"\"Evaluate counting accuracy against ground truth\"\"\"\\n    \\n    images_path = os.path.join(dataset_path, split, \"images\")\\n    labels_path = os.path.join(dataset_path, split, \"labels\")\\n    \\n    if not os.path.exists(images_path) or not os.path.exists(labels_path):\\n        print(f\"‚ùå {split} set not found\")\\n        return None\\n    \\n    image_files = [f for f in os.listdir(images_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\\n    \\n    results = []\\n    absolute_errors = []\\n    relative_errors = []\\n    \\n    print(f\"üìä Evaluating counting accuracy on {len(image_files)} images...\")\\n    \\n    for img_file in image_files:\\n        # Load image\\n        img_path = os.path.join(images_path, img_file)\\n        \\n        # Get predicted count\\n        predicted_count = box_counter.detect_boxes(img_path)\\n        \\n        # Get ground truth count\\n        label_file = img_file.replace('.jpg', '.txt').replace('.png', '.txt').replace('.jpeg', '.txt')\\n        label_path = os.path.join(labels_path, label_file)\\n        \\n        ground_truth_count = 0\\n        if os.path.exists(label_path):\\n            with open(label_path, 'r') as f:\\n                ground_truth_count = len(f.readlines())\\n        \\n        # Calculate errors\\n        absolute_error = abs(predicted_count - ground_truth_count)\\n        relative_error = absolute_error / max(ground_truth_count, 1) * 100\\n        \\n        results.append({\\n            'image': img_file,\\n            'predicted': predicted_count,\\n            'ground_truth': ground_truth_count,\\n            'absolute_error': absolute_error,\\n            'relative_error': relative_error\\n        })\\n        \\n        absolute_errors.append(absolute_error)\\n        relative_errors.append(relative_error)\\n    \\n    # Calculate summary statistics\\n    mean_absolute_error = np.mean(absolute_errors)\\n    mean_relative_error = np.mean(relative_errors)\\n    perfect_matches = sum(1 for e in absolute_errors if e == 0)\\n    accuracy = perfect_matches / len(results) * 100\\n    \\n    print(f\"\\\\nüìà Counting Accuracy Results:\")\\n    print(f\"  Perfect matches: {perfect_matches}/{len(results)} ({accuracy:.1f}%)\")\\n    print(f\"  Mean Absolute Error: {mean_absolute_error:.2f}\")\\n    print(f\"  Mean Relative Error: {mean_relative_error:.1f}%\")\\n    print(f\"  Max Absolute Error: {max(absolute_errors)}\")\\n    print(f\"  Min Absolute Error: {min(absolute_errors)}\")\\n    \\n    return {\\n        'results': results,\\n        'perfect_matches': perfect_matches,\\n        'accuracy': accuracy,\\n        'mean_absolute_error': mean_absolute_error,\\n        'mean_relative_error': mean_relative_error\\n    }\\n\\n# Run evaluations\\nprint(\"üöÄ Starting model evaluation...\")\\n\\n# Standard YOLO validation\\nval_results = evaluate_model_performance(trained_model, DATASET_PATH)\\n\\n# Custom counting accuracy evaluation\\ncounting_results = evaluate_counting_accuracy(box_counter, DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8344cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize evaluation results\n",
    "def plot_evaluation_results(counting_results):\\n    \"\"\"Plot evaluation metrics and error distribution\"\"\"\\n    \\n    if not counting_results:\\n        print(\"‚ùå No counting results to plot\")\\n        return\\n    \\n    results = counting_results['results']\\n    \\n    # Create subplots\\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\\n    \\n    # 1. Predicted vs Ground Truth\\n    predicted = [r['predicted'] for r in results]\\n    ground_truth = [r['ground_truth'] for r in results]\\n    \\n    axes[0, 0].scatter(ground_truth, predicted, alpha=0.6)\\n    axes[0, 0].plot([0, max(ground_truth + predicted)], [0, max(ground_truth + predicted)], 'r--', label='Perfect prediction')\\n    axes[0, 0].set_xlabel('Ground Truth Count')\\n    axes[0, 0].set_ylabel('Predicted Count')\\n    axes[0, 0].set_title('Predicted vs Ground Truth')\\n    axes[0, 0].legend()\\n    axes[0, 0].grid(True)\\n    \\n    # 2. Absolute Error Distribution\\n    absolute_errors = [r['absolute_error'] for r in results]\\n    axes[0, 1].hist(absolute_errors, bins=20, alpha=0.7, color='orange')\\n    axes[0, 1].set_xlabel('Absolute Error')\\n    axes[0, 1].set_ylabel('Frequency')\\n    axes[0, 1].set_title('Distribution of Absolute Errors')\\n    axes[0, 1].grid(True)\\n    \\n    # 3. Error vs Ground Truth Count\\n    axes[1, 0].scatter(ground_truth, absolute_errors, alpha=0.6, color='red')\\n    axes[1, 0].set_xlabel('Ground Truth Count')\\n    axes[1, 0].set_ylabel('Absolute Error')\\n    axes[1, 0].set_title('Error vs Ground Truth Count')\\n    axes[1, 0].grid(True)\\n    \\n    # 4. Summary Statistics\\n    stats_text = f\\\"\\\"\\\"\\nPerfect Matches: {counting_results['perfect_matches']}/{len(results)} ({counting_results['accuracy']:.1f}%)\\nMean Absolute Error: {counting_results['mean_absolute_error']:.2f}\\nMean Relative Error: {counting_results['mean_relative_error']:.1f}%\\nMax Error: {max(absolute_errors)}\\nMin Error: {min(absolute_errors)}\\n\\\"\\\"\\\"\\n    \\n    axes[1, 1].text(0.1, 0.5, stats_text, transform=axes[1, 1].transAxes, \\n                   fontsize=12, verticalalignment='center',\\n                   bbox=dict(boxstyle=\"round\", facecolor='lightblue'))\\n    axes[1, 1].set_title('Summary Statistics')\\n    axes[1, 1].axis('off')\\n    \\n    plt.tight_layout()\\n    plt.suptitle('Model Evaluation Results', fontsize=16, y=1.02)\\n    plt.show()\\n\\n# Plot evaluation results\\nif counting_results:\\n    plot_evaluation_results(counting_results)\\nelse:\\n    print(\"‚ö†Ô∏è No counting results available for plotting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5643fa",
   "metadata": {},
   "source": [
    "## 9. Deploy Model for Real-time Box Counting\n",
    "\n",
    "Implement real-time inference pipeline for processing video streams or batch images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model for deployment\\ndef save_deployment_model(model_path, output_dir=\"../models\"):\\n    \"\"\"Save model and configuration for deployment\"\"\"\\n    \\n    os.makedirs(output_dir, exist_ok=True)\\n    \\n    # Copy best model\\n    if os.path.exists(model_path):\\n        deployment_model_path = os.path.join(output_dir, \"box_detection_model.pt\")\\n        import shutil\\n        shutil.copy2(model_path, deployment_model_path)\\n        print(f\"‚úÖ Model saved to: {deployment_model_path}\")\\n        \\n        # Save model configuration\\n        config = {\\n            'model_path': 'box_detection_model.pt',\\n            'model_type': MODEL_SIZE,\\n            'num_classes': NUM_CLASSES,\\n            'class_names': CLASS_NAMES,\\n            'confidence_threshold': 0.5,\\n            'iou_threshold': 0.45,\\n            'image_size': 640\\n        }\\n        \\n        config_path = os.path.join(output_dir, \"model_config.json\")\\n        with open(config_path, 'w') as f:\\n            json.dump(config, f, indent=2)\\n        \\n        print(f\"‚úÖ Configuration saved to: {config_path}\")\\n        return deployment_model_path\\n    else:\\n        print(f\"‚ùå Model not found at: {model_path}\")\\n        return None\\n\\n# Real-time inference class\\nclass RealTimeBoxCounter:\\n    \"\"\"Real-time box counting system\"\"\"\\n    \\n    def __init__(self, model_path, confidence=0.5):\\n        self.model = YOLO(model_path)\\n        self.confidence = confidence\\n        self.frame_count = 0\\n        self.total_detections = 0\\n        \\n    def process_video(self, video_path, output_path=None, display=True):\\n        \"\"\"Process video file for box counting\"\"\"\\n        \\n        cap = cv2.VideoCapture(video_path)\\n        \\n        if not cap.isOpened():\\n            print(f\"‚ùå Cannot open video: {video_path}\")\\n            return\\n        \\n        # Get video properties\\n        fps = int(cap.get(cv2.CAP_PROP_FPS))\\n        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\\n        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\\n        \\n        print(f\"üì∫ Processing video: {width}x{height} @ {fps}fps, {total_frames} frames\")\\n        \\n        # Setup video writer if output path provided\\n        out = None\\n        if output_path:\\n            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\\n            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\\n        \\n        frame_results = []\\n        \\n        try:\\n            while True:\\n                ret, frame = cap.read()\\n                if not ret:\\n                    break\\n                \\n                self.frame_count += 1\\n                \\n                # Convert to RGB for YOLO\\n                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\\n                \\n                # Run detection\\n                results = self.model(frame_rgb, conf=self.confidence)\\n                \\n                # Count boxes\\n                box_count = 0\\n                if results[0].boxes is not None:\\n                    box_count = len(results[0].boxes)\\n                    \\n                    # Draw bounding boxes\\n                    for box in results[0].boxes:\\n                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\\n                        conf = box.conf[0].cpu().numpy()\\n                        \\n                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\\n                        cv2.putText(frame, f'{conf:.2f}', (x1, y1-10), \\n                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\\n                \\n                self.total_detections += box_count\\n                \\n                # Add frame info\\n                info_text = f\"Frame: {self.frame_count}/{total_frames} | Boxes: {box_count}\"\\n                cv2.putText(frame, info_text, (10, 30), \\n                          cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\\n                \\n                frame_results.append({\\n                    'frame': self.frame_count,\\n                    'box_count': box_count\\n                })\\n                \\n                # Write frame if output specified\\n                if out:\\n                    out.write(frame)\\n                \\n                # Display frame\\n                if display:\\n                    cv2.imshow('Box Detection', frame)\\n                    if cv2.waitKey(1) & 0xFF == ord('q'):\\n                        break\\n                \\n                # Progress update\\n                if self.frame_count % 30 == 0:\\n                    progress = (self.frame_count / total_frames) * 100\\n                    print(f\"Progress: {progress:.1f}% - Avg boxes/frame: {self.total_detections/self.frame_count:.1f}\")\\n        \\n        finally:\\n            cap.release()\\n            if out:\\n                out.release()\\n            if display:\\n                cv2.destroyAllWindows()\\n        \\n        print(f\"‚úÖ Video processing complete!\")\\n        print(f\"üìä Total frames: {self.frame_count}\")\\n        print(f\"üì¶ Total detections: {self.total_detections}\")\\n        print(f\"üìà Average boxes per frame: {self.total_detections/self.frame_count:.2f}\")\\n        \\n        return frame_results\\n    \\n    def process_webcam(self, camera_id=0):\\n        \"\"\"Process live webcam feed\"\"\"\\n        \\n        cap = cv2.VideoCapture(camera_id)\\n        \\n        if not cap.isOpened():\\n            print(f\"‚ùå Cannot open camera {camera_id}\")\\n            return\\n        \\n        print(\"üì∑ Starting webcam feed. Press 'q' to quit.\")\\n        \\n        try:\\n            while True:\\n                ret, frame = cap.read()\\n                if not ret:\\n                    break\\n                \\n                # Convert to RGB for YOLO\\n                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\\n                \\n                # Run detection\\n                results = self.model(frame_rgb, conf=self.confidence)\\n                \\n                # Count and draw boxes\\n                box_count = 0\\n                if results[0].boxes is not None:\\n                    box_count = len(results[0].boxes)\\n                    \\n                    for box in results[0].boxes:\\n                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\\n                        conf = box.conf[0].cpu().numpy()\\n                        \\n                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\\n                        cv2.putText(frame, f'Box: {conf:.2f}', (x1, y1-10), \\n                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\\n                \\n                # Add info text\\n                cv2.putText(frame, f'Boxes Detected: {box_count}', (10, 30), \\n                          cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\\n                cv2.putText(frame, \\\"Press 'q' to quit\\\", (10, frame.shape[0] - 20), \\n                          cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\\n                \\n                # Display frame\\n                cv2.imshow('Real-time Box Detection', frame)\\n                \\n                if cv2.waitKey(1) & 0xFF == ord('q'):\\n                    break\\n        \\n        finally:\\n            cap.release()\\n            cv2.destroyAllWindows()\\n\\n# Save the deployment model\\ndeployment_model_path = save_deployment_model(BEST_MODEL_PATH)\\n\\n# Create deployment instructions\\ndeployment_instructions = f\\\"\\\"\\\"\\n# üöÄ Deployment Instructions\\n\\n## Model Files\\n- **Model**: `{deployment_model_path or 'box_detection_model.pt'}`\\n- **Config**: `../models/model_config.json`\\n\\n## Usage Examples\\n\\n### 1. Single Image Processing\\n```python\\nfrom ultralytics import YOLO\\n\\n# Load model\\nmodel = YOLO('../models/box_detection_model.pt')\\n\\n# Detect boxes\\nresults = model('image.jpg', conf=0.5)\\nbox_count = len(results[0].boxes) if results[0].boxes else 0\\nprint(f\\\"Detected {{box_count}} boxes\\\")\\n```\\n\\n### 2. Batch Processing\\n```python\\n# Process multiple images\\nresults = model(['image1.jpg', 'image2.jpg'], conf=0.5)\\nfor i, result in enumerate(results):\\n    count = len(result.boxes) if result.boxes else 0\\n    print(f\\\"Image {{i+1}}: {{count}} boxes\\\")\\n```\\n\\n### 3. Real-time Video Processing\\n```python\\n# Initialize real-time counter\\ncounter = RealTimeBoxCounter('../models/box_detection_model.pt')\\n\\n# Process video file\\ncounter.process_video('input_video.mp4', 'output_video.mp4')\\n\\n# Or process webcam feed\\ncounter.process_webcam(0)  # Use camera 0\\n```\\n\\n## Integration with Web Application\\nThe trained model can be integrated with the Streamlit web application:\\n\\n1. Copy the model file to the `models/` directory\\n2. Update the `MODEL_PATH` in `.env` or `config.yaml`\\n3. Run the web application: `streamlit run app.py`\\n\\n## API Integration\\nFor production deployment, consider:\\n- FastAPI for REST API endpoints\\n- Docker containerization\\n- GPU acceleration setup\\n- Model optimization (TensorRT, ONNX)\\n\\\"\\\"\\\"\\n\\nprint(deployment_instructions)\\n\\n# Save deployment instructions\\nwith open('../DEPLOYMENT.md', 'w') as f:\\n    f.write(deployment_instructions)\\n\\nprint(\\\"\\\\n‚úÖ Deployment files and instructions created!\\\")\\nprint(\\\"üìÑ See DEPLOYMENT.md for detailed usage instructions\\\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
